\documentclass[a4paper,11pt]{article}
\usepackage{amsmath}
\usepackage{fancyhdr}
\usepackage{graphicx}
\usepackage{url}

\usepackage[margin=1in]{geometry}
%\usepackage[top=.6in, bottom=.8in, left=.8in, right=.8in]{geometry}
%==== Insert cool image between title and authors ====%
\title{Using 3D Models and Computer Vision Algorithms to \\ Implement Monte Carlo Localization}
\author{ \\[7in]  John Allard, Alex Rich \\ 2014 Summer Computer Science REU, Harvey Mudd College\thanks{Funded By The NSA}}
%\date{July 6th, 2014 \\}

\begin{document}

% ==== Title Page ==== %
  \maketitle   
  %\thispagestyle{empty}
  \newpage  
  
% ==== Table Of Contents ==== %
  \tableofcontents
  \newpage

  % ==  Paper Abstract   == %
  \begin{abstract}  
  Our research group attempts to implement the Monte Carlo Localization algorithm using a 3D model of an environment and a stream of images from a robot in that environment. This entails the use of various computer vision algorithms to find and compare features between the image feed and our 3D model. Objectives of the project include runtime speed, accuracy, versatility, and ease of use. This paper outlines the process that our research group has undertaken to accomplish this task and an analysis of our resulting program. 
  \end{abstract}
  
%====================================%
%===== Section 1, Introduction ===== %
%====================================%
  \section{Introduction} 
 The Monte Carlo Localization (MCL) algorithm has been used in the past\footnote{ Dieter Fox, et al. Carnegie Mellon University, University of Bonn.} to successfully localize robots using 2D maps of an environment and a stream of range sensor information. We take this algorithm and implement it in a 3D map and four dimensions ($x, y, z, \theta$) using color images as sensor data. The end goal of this project is to place a robot somewhere in an environment, request that it go somewhere, and have the robot determine where it is and how to get to the desired location.

The selection of a camera as the sensor reflects the cheapness and availability of cameras, and the assumption that most robots have cameras and not necessarily range data sensors.
  
% ==   1.1 Process Overview  == %
  \section{Monte Carlo Localization Process Overview}
%  \emph{This section is for those unfamiliar with the Monte Carlo Localization algorithm.}

The overall process of having an actor\footnote{An actor is any device that has sensors and can move around an environment, e.g. a robot.} localize itself in an environment via the MCL algorithm is comprised of many steps.
\subsection{Pre-Localization}
 These are requirements that must be satisfied for MCL to work.
  \begin{enumerate}
  \item A map of the environment needs to be imported or constructed.
  \item Some set of quantifiable features about the map must be chosen. This enables a comparison between sensor readings from the actor and expected sensor readings from different places in the map.
  % \item This step is optional.  Features from the map are computed from many different reference points and are stored for use during the localization attempt. This limits the possible particle locations compared to generating feature data at run-time, but it reduces the computational resources needed significantly and allows one to simply look up the data in a map or container. Pre-computing features also significantly reduces the complexity of the localization source code.
  \end{enumerate}
\subsection{During Localization}
  These are the steps that the Monte Carlo Localization goes through to determine the location of an actor. Each step can be implemented in many different ways.
  \begin{enumerate}
  \item An actor is placed in the environment, and some number of guesses of where it might be are randomly generated.\footnote{The uniformity of this distribution depends on the user's previous knowledge of the actors location.} These `guesses' we will call particles, and each particle is a data structure that contains information about its own current perspective in the environment and the sensor data it would expect to read from that perspective.
  \item The program compares the current sensor readings from the actor to the expected sensor readings for each particle, and assigns a weight to each particle based on how strongly the readings correspond to one another.
  \item A distribution is created according to the grouping and weighting of particles in the program. The more heavily weighted a particle is, the higher probability it will be sampled from our distribution.
  \item A new set of particles are sampled from this distibution, as well as from a uniform distribution across the map. After this step the total number of particles in the program is the same as in the last step.
  \item The actor is moved to a new point in the environment via some movement command from the program. Each particle has its perspective updated according to the same commands, plus some statistical error from the uncertainty in the actor's movements. The expected sensor readings for each particle are also updated to correspond to its new perspective of the environment.
  \item Steps 2-4 are repeated until the localization is stopped. 
  \end{enumerate} 
  
The power of this algorithm comes from the mobility of the particles. When the actor moves, the particles move, and thus they store the probability that the actor has traveled in the path that the particle has taken. Over time, impossible paths will cause incorrect particles to be removed from the current set of guesses, leaving room for better guesses to take their place. This results in the gradual merging of particles in space to the correct location.

%=====================================%
%===== Section 2, 3D Map Building ====%
%=====================================%
  \section{Building the 3D Map}
  We used a 3-dimensional model as our map of the actor's environment. The models tested were fully textured, high-quality laser scans of a room or series of rooms and hallways. The 3D model was important because we intended on using cameras as our sensors for the actor in our environment. The 3D map was built using a high quality camera and range scanner donated by Matterport. The camera interfaces with an iPad and scans its surroundings from multiple vantage points, eventually stitching together everything it sees into one map. %Building the 3D map would have been nearly impossible without the generosity of the Matterport team and specifically their COO Mike Beebe. They gave our research team a very well built and user-friendly 3D imaging camera which uses laser-range data to scan an enclosed space. The use of this camera saved our team countless headaches that would have incurred by attempting to stitch Kinect range-data together.
  
  \subsection{Taking the Scans}
 Our team took 42 scans of the space we work in, a roughly 3500 sqft floor consisting of 7-8 rooms and a large amount of non-static objects, such as chairs, robots, and whiteboards. The Matterport software allows about 100 scans for a single model, and the software bundled with the camera automatically merges these scans into one textured mesh. The software also automatically converts the data to a .obj file format for viewing on their site. We were then able to use this object file to determine features about our map.

  \subsection{Filling In the Gaps}
  Our model was inevitably left with gaps from places that were obstructed from the view of our camera. To help improve the overall quality of our map we used the Meshlab software. This allowed us to remove disconnected pieces, remove unreferenced vertices, and smoothen out some jagged areas. on top of this, we rendered the background color pink to allow us to single out features on these areas and remove them from the program.
  
  \subsection{Creating a Database of Images from the Map}
  Our goal is to use computer vision algorithms that compare between 2D images, which meant that we had to represent our 3D map in 2D for our feature matching algorithms to work properly. To represent our 3D map as 2D information, we decided to render images from thousands of different perspectives within our map. The following steps were taken to accomplish this.
  \begin{enumerate}
  \item Establish a bounding box around our map.
  \item Define a plane that sits above and parallel to the x-y plane of our map.
  \item Define the scale of a grid imposed on this plane.
  \item Go to each grid location and render images from 8 perspectives, rotating from 0 to 360 degrees.
  \item Name the images according to their location in the map and store them for later use.
  \end{enumerate}

  This process allows us to turn our 3D model into a catalogued database of 2D images from a large amount of places within our model. From here we can use existing computer vision related algorithms (SURF, SIFT, ORB, etc.) to do the matching and weighing between places in our 3D model and the incoming image feed from the actor in the environment. 

  Once the database of images was computed, the next step was computing another database of computer-vision derived features from these images.


%=======================================================%
%===== Section 3, Computing Features from the Map ===== %
%=======================================================%
  \section{Computing Features from the Map}
Once we have a database of images, we must go through each image and compute a set of general feature data about each picture. This is done before localization because computing features is fairly computationally intensive, and doing so during runtime slows localization down. During the boot phase of localization, we load all features into a map.

  \subsection{Types of Features}
There are a variety of features that can be used when describing an image. In its current state, the project uses extracted SURF features as well as grayscale and black and white images.

SURF (Speeded Up Robust Features) is a feature detection algorithm that detects similar features as SIFT (Scale-Invariant Feature Transform). SURF detects interesting keypoints in an image. Using OpenCV, we can detect these points, describe them, and eventually compare two keypoints.

The Grayscale image is a highly coarse image that simply splits an image into a grid, then computes the average intensity inside each grid square. From this image, we can compute the black and white ``above below" image. This is created by finding the average intensity of the grayscale, then determining if a square is either higher or lower than this. If a square is higher, it is colored white, otherwise it is colored black.

  \subsection{Storing the Features}
The precomputed features are stored in a yaml file. OpenCV has built in storage methods, which makes this a simple process. Because each perspective has its own file for storing keypoints|in addition to its own file for grayscale and black and white images|the filename contains the meta information about each description. The program is able to load all features and know their corresponding location and orientation by first looking inside the folder to see what perspectives are available, then load into the map.

%========================================%
%===== Section 4, MCL Implementation ====% 
%========================================%
  \section{Implementation}
   
 \subsection{Actor}
The actor in our MCL implementation satisfies three constraints: communicate with the Robot Operating System (ROS), move autonomously, and publish image data from a camera. During our testing, we used several actors, including the Parrot AR Drone, the iCreate Robot, and a virtual robot. The actor also should be able to navigate in its environment. In our implementation, simple point to point navigation was sufficient, however a future goal is to implement the ability to make smart paths allowing the robot to go through doors and around or over tables.

\subsection{Assigning Weights}
Each possible location in the environment is described by a discrete particle. The particle contains a weight associated with the probability that the actor is currently at that particle in the real world. When the actor moves, every particle moves as well. During each iteration, the program reevaluates by weighing every particle sampled from the distribution. Using the stored features, computer vision algorithms determine the similarity between the actor image data and the image data at each particle.

There are several possible ways two images can be compared to each other. Issues with scaling, lighting, and photo quality can make comparing images difficult. In its current implementation, our algorithm only uses computed SURF features. OpenCV has methods to match descriptors in two images. Using the $k$ nearest neighbors matching algorithm with $k = 2$ and the ratio test\footnote{See Lowe, David G. ``Distinctive Image Features from Scale-Invariant Keypoints." {\it International Journal of Computer Vision} (2004). pg 19-20.  \url{http://www.cs.ubc.ca/~lowe/keypoints/}.} we get some set of matched keypoints. We want to get some kind of score for how ``good" the matches are. Each match (stored as a \verb.DMatch. in OpenCV) contains a member (\verb.distance.) that indicates the goodness of a single match. A weighting for each pair of images can be determined by
\[
	\mathtt{weight} = \sum_{i} \frac1{\mathtt{matches}[i].\mathtt{distance} + 0.8}
\]
where \verb.matches. is a vector of \verb.DMatches.. This formula has several benefits. First, it rewards pairs of images that have many good matches. Recall that the higher the similarity, the higher the probability that the two images are the same. If two images have many matches (as two similar images ought to), \verb`matches.size()` will be high and there will be many terms in the summation. Secondly, it rewards matches that are {\it good}. If a match is a good match, it will have a low \verb.distance.. 

Another formula that does well is the following:
\[
\mathtt{weight} = \mathtt{matches.size()} - 10 \times \frac{\sum_{i} \mathtt{matches}[i].\mathtt{distance}}{\mathtt{matches.size()} + 1}
\]
This also rewards images that have many good matches with the first term, then penalizes pairs of images where the average match distance is high.

\subsection{Determing the Location}
The list of weighted particles has been computed and can now help locate the actor. There are a few ways the location can be determined: Top Match, Weighted Average, or a combination of the two. The Top Match is simply the perspective that has the highest weight. This is a fairly good estimate, however it disregards much of the information that the MCL algorithm provides, such as the weights of all other perspectives. 

The second option is the Weighted Average, which sums up all of the locations, weighted by their probaility, and finds the average. Once the average position is found, the locating algorithm only looks at nearby particles to determine the actor's orientation. This is also good, but sometimes does not make sense, for example if there is a square region in the middle of a room that a actor cannot possibly be, it is possible to return a location inside this region. To combat this issue, we use the Top Match when the Weighted Average is at an impossible location.

\subsection{Communicating with Actor}
Using ROS, the Localization program publishes several commands: location data and movement requests. By publishing location data, the actor knows (has an idea of) where it is in the map and can then figure out where to go. When the actor receives a movement request, it determines its move, consisting of a turn and a translation, publishes the move, moves, then publishes a message stating that it is done moving.

\subsection{Visualization}
For the user to see how the localization program is working, various visualization assistants were constructed to accompany the program. A user is able to see what the robot sees, a top-down view of the map showing the location and relative weights of each particle, the Top Match and Weighted Average chosen by the program, and a three dimensional visualizer showing all the particles within the actual map.

\section{Results}
We can analyze how our program performs for each of the initial objectives.

\subsection{Runtime Speed}
Depending on the type of feature matching being performed and the number of particles currently active, our algorithm takes less than one second to determine a guess for the location of the actor. When the number of particles surpass 700, this time begins to noticably take longer than a second. We consider the algorithm to be working in real time because it usually takes longer for the robot to make its move than the algorithm takes in each iteration.

\subsection{Accuracy}


\subsection{Versatility}
Our algorithm is versatile to the extent that a map exists. It works with any robot that communicates with ROS. As long as a map of an environment exists in .obj file format, the program is able to localize a stream of image data to the best of its ability.

\subsection{Ease of Use}


\section{Issues}
\subsection{Dynamic Environments}
In our implementation we assume a static environment. This is not always the case, as there are often people inhabiting the same space as robots. Also, objects like chairs, coffee cups, and even tables are not guaranteed to remain in the same position for any period of time. Future work will involve locating objects that are deemed nonstatic and ignoring them in the weighing process. Moving objects, such as a person walking, ought to also be ignored.

\subsection{Navigation}
It is certainly possible to implement a smarter navigation algorithm in which a robot knows where walls, doors, and tables are. In our current implementation the robot merely travels in a straight line from its location to the destination, however this can cause it to bump into tables or crash into walls. A smart navigation algorithm would allow the robot to pass throuh a door and, if it were a drone, to fly over tables.

\subsection{Weighing}
When each particle's image data is compared with that of the robot, there are many different ways to arrive at a similarity score. Currently we use only the number of feature matches and the ``goodness" of each match. There are, however ways to incorporate algorithms such as RANSAC (Random Sample Consensus) to consider how well the matches align with each other. In our attempts at using this strategy, however, we found it was difficult to find one formula that will consistently weigh pairs of images with different match quantities. In addition, there may be quick image comparison techniques available to relate the black and white and gray scale images.

\section{Conclusions}
What a dumb robot.








  








\end{document}