\documentclass[a4paper,11pt]{article}
\usepackage{amsmath}
\usepackage{fancyhdr}
\usepackage{graphicx}
\usepackage[top=.6in, bottom=.8in, left=.8in, right=.8in]{geometry}
%==== Insert cool image between title and authors ====%
\title{Using 3D Models and Computer Vision Algorithms to \\ Implement Monte Carlo Localization}
\author{ \\[7in]  John Allard, Alex Rich \\ 2014 Summer Computer Science REU, Harvey Mudd College}
\date{July 6th, 2014}


\begin{document}


% ==== Title Page ==== %
  \maketitle   
  \newpage  
  
% ==== Table Of Contents ==== %
  \tableofcontents
  
  \newpage
  
%====================================%
%===== Section 1, Introduction ===== %
%====================================%
  \section{Introduction} 
  
% ==    1.1 Paper Abstract   == %
  \begin{abstract}  
  The Monte Carlo Localization (MCL) algorithm has been used in the past\footnote{ Dieter Fox, et al. Carnegie Mellon University, University of Bonn.} to successfully localize robots using 2D maps of an environment and a stream of range  sensor information. Our research group is attempting to implement the Monte Carlo Localization algorithms  using a 3D model of the environment and a feed of images from a robot in that environment. This entails the use of various computer vision algorithms to find and compare features between the image feed and our 3D model. This paper will outline the overall processes that our research group has undertaken to accomplish this task and an analysis of our resulting program. 
  \end{abstract}
  
% ==   1.2 Process Overview  == %
  \subsection{MCL Process Overview}
  \emph{This section is for those unfamiliar with the Monte Carlo Localization algorithm.}\\ The overall process of having an actor\footnote{Any device that has sensors and can move around an environment} localize itself in an environment via the MCL algorithm is comprised of many steps. A simplified outline of these steps can be stated as follows.\\
  
  Pre-Localization Attempt :
  
  \begin{enumerate}
  \item A map of the environment needs to be imported or constructed.% This can be a 2D top-down map, a 3D map created with a 
  %laser range sensing camera, or any other type of map that contains  
  \item Some set of quantifable features about the map must be chosen, this enables a comparison between sensor readings from the actor and expected sensor readings from different places in the map.
  \end{enumerate}
  
  During Localization Attempt :
  
  \begin{enumerate}
  \item An actor is let loose in the environment, and a large amount of guesses as to where it could be are randomly generated according to some distribution\footnote{This distribution depends on the users previous knowledge of the actors location.}. These 'guesses' are called particles, and each particle is a data structure that contains information about its current perspective in the environment and the sensor data it would expect to read from that perspective.
  \item The program compares the current sensor readings from the robot to the expected sensor readings for each particle, and assigns a weight to each particle based on how strongly the readings correspond to one another.
  \item Particles with low weights are thrown out, and a small amount of randomly generated particles are added to the program.
  \item The robot is moved to a new point in the environment via some movement commands from the program. Each particle that still exists has its perspective updated according to the same commands. The expected sensor readings for each particle is also updated to correspond to the its perspective of the environment. 
  \item The process of comparing sensor readings from the robot to those of the various particles, assigning weights, culling low weights, and moving to a different location in the map is repeated until the particles converge on a single location (In theory, at least).
  \end{enumerate} 
  
  \newpage
  
%==========================================%
% ===== Section 2, Our Implementation ==== %
%==========================================%
  \section{Our Implementation}
  
% === Subsection 2.1 - Differences from Previous Implementations === %
  \subsection{Differences from Previous Implementations}
  Our implementation of the MCL algorithm is different from ones done before in a few ways.
  
  \begin{enumerate}
  \item We are using a full scale 3 dimensional model of a space as our map. This model was made with the help of the Matterport 3D Imaging Camera \footnote{See section 3, \emph{Building the 3D Model}.}.
  \item The majority of the MCL examples we have seen on the internet use range-sensor data and 2 dimensional maps to do the feature comparison and weighting. We have decided to try a pure computer-vision based approach. 
  \item We will be pre-computing the features from thousands of different perspectives within our 3D model for quick data access when creating particles. This is different than approaches that have previously been taken, where they take a particles position and compute the expected sensor data in real-time for comparison against the incoming sensor data feed from the actor.
  \item We hope to have our systems work with any robots that implement a specific ROS\footnote{\emph{ROS} - Robot Operating System } interface. This will require that the robots can publish image data at a certain rate and that they can accept pre-defined movement commands from our program.
  \end{enumerate}
  
  
%=== Subsection 2.2 - Pre-Computation === %
  \subsection{Pre-Computation}
  
  We decided that our environment would be the 2nd floor of the Sprague building at Harvey Mudd College. This environment is conveient because it happens to be where our group works. This is a large space consisting of 7-8 semi-connected rooms, with lots of chairs, computers, and other items that complicate the topography of our space. 
  
  \begin{enumerate}
  \item We began by using our Matterport camera to take 42 separate scans of the floor from different locations. The Matterport software automatically merges these different scans into one entire map. The map that we ended up with was quite detailed with both polygon and texture data, but ultimately some areas were obstructed and so it was not perfect. 
  \item The obstructed areas of our map were marked with an odd color and exluded from any feature matching algorithms as to minimize the false-positive matches that would result from holes in our map giving the particles a view of items that should be obstructed.
  \item Images from thousands of different perspectives within our 3D model are generated. An effort is made to only generate images from \emph{meaningful} perspectives, such as excluding images from within walls or staring straight towards the ceiling. 
  \item Many different types of features\footnote{Detected with SURF, SIFT, ORB, Harris, or other feature detection algorithms} are computed for each image that was generated in step 3. This allows us to simply look up the features for a specific particle instead of having to compute them in real-time.
  \item 
  \end{enumerate}
  
  
%=== Subsection 2.3 - During Localization ===%
  \subsection{During Localization}
  \item
  \item
  




%=====================================%
%===== Section 3, 3D Map Building ====%
%=====================================%
  \section{Building the 3D Map}




%=======================================================%
%===== Section 4, Computing Features from the Map ===== %
%=======================================================%
  \section{Computing Features from the Map}




%========================================%
%===== Section 5, MCL Implementation ====% 
%========================================%
  \section{MCL Implementation}
   
  




  




\end{document}